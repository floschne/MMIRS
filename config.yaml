api:
  port: 8081
retriever:

  uniter:
    n_gpu: 1
    uniter_dir: ${env:UNITER_DATA_DIR}
    model_config: ${env:UNITER_MODEL_CONFIG}
    num_imgs: 1000
    batch_size: 50
    n_data_workers: 0
    fp16: True
    pin_mem: True

  teran:
    device: cpu
    cuda_device: -1
    model_config: configs/teran_coco_MrSw_IR_PreComp_API.yaml
    num_data_workers: 8
    model: pretrained_models/coco_MrSw.pth.tar

image_server:
  lighttp:
    https: false
    host: localhost
    port: 8082
    context_path: /
    datasources:
      - uniter: /flickr30k/
      - teran: /coco14/val2014/

preselection:
  focus:
    max_tokens: 3  # maximum number of space-separated tokens in the focus word. E.g. "small green bird"
    remove_stopwords: True
    uncased: True
    lemmatize: True
    pos_tags:
      - NOUN
      - PROPN
      - ADJ
      - VERB
    vocab:
      objs_file: data/vocab/objects_vocab.txt
      attrs_file: data/vocab/attributes_vocab.txt
    magnitude:
      embeddings: data/magnitude/crawl-300d-2M.magnitude
      top_k_similar: 25  # per focus token
      max_similar: 25  # total
    wtf_idf:
      index: data/wtf_idf/wtf_idf_octh_0.20_acth_0.15_alpha_0.95.index  # TODO generify for multiple datasets
    spacy_model: en_core_web_lg

  context:
    sbert:
      symmetric_model: paraphrase-distilroberta-base-v1
      asymmetric_model: msmarco-distilbert-base-v2
      max_seq_len: 200  # number of subwords in the captions before it gets truncated
      symmetric_embeddings:
        - wicsmmir: data/sembs/wicsmmir_symm_embs.pkl
      asymmetric_embeddings:
        - wicsmmir: data/sembs/wicsmmir_asym_embs.pkl
    faiss:
      symmetric_indices:
        - wicsmmir: data/faiss/wicsmmir_symm.faiss
      asymmetric_indices:
        - wicsmmir: data/faiss/wicsmmir_asym.faiss
      nprobe: 250  # Number of VCs to explorer at search time (tradeoff between search accuracy and search time)


