api:
  port: 10161

logging:
  max_file_size: 500 # MB
  level: DEBUG

image_server:
  datasources:
    coco:
      images_root: /srv/7schneid/datasets/coco/images
      image_prefix: COCO_000000
      image_suffix: .jpg

# ---- not supported anymore! ----
#  lighttp:
#    https: false
#    host: localhost
#    port: 10162
#    context_path: /

  pyhttp:
    https: false
    host: localhost
    port: 10162
    context_path: /
    link_root_dir: /srv/7schneid/mmirs_pyhttp_link_root
    flush_link_dir: True

preselection:
  focus:
    max_tokens: 3  # maximum number of space-separated tokens in the focus word. E.g. "small green bird"
    remove_stopwords: True
    uncased: True
    lemmatize: True
    pos_tags:
      - NOUN
      - PROPN
      - ADJ
      - VERB
    vocab:
      objs_file: data/vocab/objects_vocab.txt
      attrs_file: data/vocab/attributes_vocab.txt
    magnitude:
      embeddings: data/magnitude/crawl-300d-2M.magnitude
      top_k_similar: 25  # per focus token
      max_similar: 25  # total
    wtf_idf:
      coco:
        file: data/wtf_idf/coco_wtf_idf_octh_0.20_acth_0.15_alpha_0.95.index
        doc_id_prefix: COCO_[trainval]{3,5}2014_000000
    spacy_model: en_core_web_lg

  context:
    use_symmetric: True # if False, do not use symmetric embeddings, indices, and models
    use_asymmetric: False # if False, do not use asymmetric embeddings, indices, and models
    sbert:
      symmetric_model: paraphrase-distilroberta-base-v1
      asymmetric_model: msmarco-distilbert-base-v2
      max_seq_len: 200  # number of subwords in the captions before it gets truncated
      symmetric_embeddings:
        - coco: data/sembs/coco_symm_embs.pkl
      asymmetric_embeddings:
        - coco: data/sembs/coco_asymm_embs.pkl
    faiss:
      symmetric_indices:
        - coco: data/faiss/coco_symm.faiss
      asymmetric_indices:
        - coco: data/faiss/cooc_asym.faiss
      nprobe: 350  # Number of VCs to explorer at search time (tradeoff between search accuracy and search time)

fine_selection:
  max_workers: 32

  feature_pools:
    coco: # dataset
      teran_coco: # teran retriever name that precomputed the image embeddings. must match the retrievers from retriever section
        feats_root: /srv/7schneid/datasets/coco/pre_computed_embeddings
        fn_prefix: COCO_000000
        num_workers: 32
        pre_fetch: False
      teran_wicsmmir: # teran retriever name that precomputed the image embeddings. must match the retrievers from retriever section
        feats_root: /srv/7schneid/datasets/coco/pre_computed_embeddings_wicsmmir_v2
        fn_prefix: COCO_000000
        num_workers: 32
        pre_fetch: False
      teran_f30k: # teran retriever name that precomputed the image embeddings. must match the retrievers from retriever section
        feats_root: /srv/7schneid/datasets/coco/pre_computed_embeddings_f30k
        fn_prefix: COCO_000000
        num_workers: 32
        pre_fetch: False

  retrievers:
    teran_coco:
      retriever_type: teran
      device: cuda
      model: pretrained_models/coco_MrSw.pth.tar
      model_config: configs/teran_coco_MrSw_IR_PreComp_API.yaml

  max_focus_annotator:
    datasources:
      coco:
        bbox_root: /srv/7schneid/datasets/coco/features_36/bua
        fn_prefix: COCO_000000
        fn_suffix: .npz

    annotated_images_dst: /srv/7schneid/mmirs_annotated_images_dst

  wra_plotter:
    wra_plots_dst: /srv/7schneid/mmirs_wra_images_dst
    cell_size_px: 40


mmirs:
  pss:
    merge_op: intersection
    max_num_context_relevant: 5000
    max_num_focus_relevant: 5000
    max_num_relevant: 10000
    min_num_relevant: 1000
    focus_weight_by_sim: False
    exact_context_retrieval: False

  img_server: pyhttp
